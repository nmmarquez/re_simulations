---
output:
    html_document:
        includes:
            in_header: assets/huskyheader.html
            before_body: assets/huskynavbar.html
---
#### Neal Marquez

```{R echo=F,message=F,warning=F}
# Recall 
# p(f)ff is a Gaussian process if for any finite subset {x 1 , . . . , x n } in X , the marginal
# distribution over that finite subset p(f) has a multivariate Gaussian distribution.
#
```

## Abstract

A newfound focus on reducing inequalities in health outcomes has required that data from extremely granular locations be used to understand the geographic risk field of detrimental health outcomes and who lies at the margins. While some data sources provide geographic coordinates of the area that individuals were surveyed, many more data points are representative of larger areal units. How to harmonize these data sources has been widely contested but to date no solution has overcome issues with changing areal units and point data. We present a new approach using an approximated integration of a continuous underlying field to incorporate areal units of data of any shape alongside point data. This approach is applicable to many non-linear models and measures of responses. To test this approach we compare our methodology against previously stated solutions in a simulation environment where the underlying structure of the "risk field" is known. In addition we provide an example of the implementation of our approach using Demographic and Health Survey data of under 5 mortality in the Dominican Republic. We find that ...

## Methods

### Estimating Values from Points and Polygons  
Let's us assume that there is a continuous random variable that we observe that takes place on a 2 dimensional spatial field $s$. The random variable is correlated over space and follows a Gaussian Process with a constant mean function and a Mat√©rn covariance function along with additional random white noise. 

#### Underlying Data Generating Process
$$
\boldsymbol{Y} \sim \mathcal{GP}(\mu_\text{Constant}(\beta_0), K_{\text{Matern}}(\kappa, \sigma_\eta, \nu) + K_{\text{White}}(\sigma_\epsilon)) \\
$$

Lindgren et al 2011 show that a Gaussian Field, which is continuously indexed, can be well represented by a Gaussian Markov Random Field, which is discretely indexed. If we observe a number of points in the set $\{1, \dots, n\}$ we can estimate the underlying field with linear form shown below. 

#### Simple Linear Spatial Model
$$
i,j \in \{1, \dots, n\}\\
Y_i \sim \mathcal{N}(\beta_0 + \eta(s_i), \sigma_\epsilon) \\
\boldsymbol{\eta} \sim \text{MVN}(\boldsymbol{0}, \boldsymbol{\Sigma}) \\
\Sigma_{ij} = \frac{\sigma^2_\eta}{2^{\nu-1} \Gamma(\nu)}
    (\kappa ||s_i - s_j||)^\nu K_\nu(\kappa ||s_i - s_j||) \\
\Sigma_{ij} = \text{Cov}(\eta(s_i) , \eta(s_j)) \\
$$

$$
i,j \in \{1, \dots, n\}\\
Y_i \sim \mathcal{GP}(\mu_\text{Constant}(\beta_0), \kappa_{\text{Matern}}(\kappa, \sigma_\eta, \nu) + \kappa_{\text{White}}(\sigma_\epsilon)) \\
$$

In this model $\sigma_\epsilon$ represents the iid white noise variance, $\beta_0$ is the constant mean function, $s_i$ is the index in space for point $i$, and $Y_i$ is the observed value. $\eta$ represents the latent effects of the continuous spatial field. Each index on the spatial field covaries with every other point on the spatial field as dictated by the Multivariate Normal Distribution with a covariance matrix $\Sigma$. $\Sigma$ it self has several parameters that govern its structure $\kappa$, $\sigma_\eta$, and $\nu$.

The models parameters may be estimated using one of several hierarchical modeling approaches and once done the ability to predict to new locations on the spatial field that have not yet been observed is trivial as explained in Lindgren et al. If however we observe a point that we don't know the exact location of the observation but know the area $\mathcal{A}$ the point comes from we may still leverage the underlying spatial field by using the definite integral of the latent field which is captured by $\mathcal{A}$. An underlying assumption to this process is the each point in space is equally likely to be selected however we may adjust the integral to include a weighting function if this is not true.

#### Extension to Areal Units  
$$
k \in \{ 1, \dots , m \} \\
l \in \mathbb{R} \\
Y_k \sim \mathcal{N}(\hat{y}_k, \sqrt{\sigma_\epsilon^2 + f(\langle \langle \mathcal{A}_k \rangle \rangle, \kappa, \sigma_\eta, \nu)}) \\
\hat{y}_k = \beta_0 + \frac{\int_{\forall l \in \mathcal{A}_k} \eta(s_l)ds}{\langle \langle \mathcal{A}_k \rangle \rangle} \\
$$

This process is relatively straight forward in the linear model process where the mean is constant, however, the inclusion of covariates and non-linearity make this process more difficult. Nevertheless, we may walk through the process in a similar way when we have a binary outcome. First we look at the data likelihood for points in space. This process is similar to the linear analog with the exception of the inverse logit transformation of the linear additive effects of the covariates and the latent spatial effects.

#### Non Linear Binomial Likelihood of Points in Space
$$
i,j \in \{1, \dots, n\}\\
Y_i \sim \text{Binomial}(N_i, \hat{p}_i) \\
\text{logit}(\hat{p}_i) = X_i \boldsymbol{\beta} + \eta(s_i)  \\
\boldsymbol{\eta} \sim \text{MVN}(\boldsymbol{0}, \boldsymbol{\Sigma}) \\
\Sigma_{ij} = \frac{\sigma^2_\eta}{2^{\nu-1} \Gamma(\nu)}
    (\kappa ||s_i - s_j||)^\nu K_\nu(\kappa ||s_i - s_j||) \\
\Sigma_{ij} = \text{Cov}(\eta(s_i) , \eta(s_j))
$$

When we receive observations from an areal unit rather than a point we can evaluate the likelihood using the integral of the field of estimated probabilities, which is a function of the underlying latent field, rather than integrating the latent field itself. This integral is not easily derived, however, we can use the Riemann approximation of the probability field to estimate the integral. In this we may use data that comes from a known areal unit alongside spatial point data in computing the likelihood of the observed data. This process again assumes an equal chance of selecting points across the field but, just as before, a weighting function may be applied in order to adjust for the differential probabilities of selecting an area.  

#### Predictions of Evenly Distributed Population Areal Unit
$$
k \in \{ 1, \dots , m \} \\
l \in \mathbb{R} \\
Y_k \sim \text{Binomial}(N_k, \hat{p}_k) \\
\hat{p}_k = \frac{\int_{\forall l \in \mathcal{A}_k} \hat{p}_lds}{\langle \langle \mathcal{A}_k \rangle \rangle} \\
\hat{p}_k \approx \frac{\sum_l \hat{p}_l \Delta_l}{\langle \langle \mathcal{A}_k \rangle \rangle}=\frac{\sum_l \text{inv.logit}\big(X_l \boldsymbol{\beta} + \eta(s_l) \big) \Delta_l}{\langle \langle \mathcal{A}_k \rangle \rangle}
$$

## Analysis

### Simulation Primer
We begin by testing the validity of our methodology by comparing results from a traditional SPDE approach to one that combines a likelihood with approximated integration for polygons. Data are generated for a binomial outcome where the probability for an event occurring varies over space but is autocorrelated. The underlying field is generated via a Gaussian Process with a constant mean function and then logit transformed for probabilities. The model follows the binomial formula above with only one coefficient for the intercept. We will simulate this field over a portion of the United States to demonstrate how administrative units probabilities for events can be calculated.

```{r loads, message=FALSE, warning=FALSE, echo=FALSE}
rm(list=ls())
pacman::p_load(INLA, ggplot2, data.table, lattice, arm, dplyr, TMB, 
               ar.matrix, dtplyr)
set.seed(124)

# apply a set of values from the mesh to the projected surface and convert
# to a dataframe. Works for both vectors and fitted models.
mesh2DF <- function(model){
    if(class(model) == "numeric"){
        x <- model
        sdx <- rep(NA, length(x))
    }
    else{
        x <- model$z$mu
        sdx <- model$z$sd
    }
    M <- length(proj$x)
    DT <- data.table(x=rep(proj$x, M), y=rep(proj$y, each=M), 
                     obs=c(inla.mesh.project(proj, field=x)),
                     sdx=c(inla.mesh.project(proj, field=sdx)))
    expand.grid(proj$x, proj$y) %>%
        SpatialPoints(US.df@proj4string) %>%
        over(US.df) %>%
        as.data.table %>%
        cbind(DT) %>%
        mutate(aproj=1:n(), obsField=!is.na(id)) %>%
        group_by(obsField) %>%
        mutate(obsAproj=1:n()) %>%
        ungroup %>%
        as_tibble %>%
        mutate(stateID=group_indices(., STATEFP)) %>%
        mutate(stateID=ifelse(!obsField, NA, stateID)) %>%
        mutate(obsAproj=ifelse(!obsField, NA, obsAproj)) %>%
        as.data.table
}

# Do some shape cleaning
row.names(US.df@data) <- 1:nrow(US.df)
US.df$id <- 1:nrow(US.df)
USDF <- fortify(US.df, region="id") %>%
    mutate(id=as.numeric(id)) %>%
    left_join(US.df@data, by="id")

 # randomly sample from the space to build first mesh and true underlying field
randomSPDF <- spsample(US.df, 800, "random")
randomSPDF$long <- randomSPDF@coords[,1]
randomSPDF$lat <- randomSPDF@coords[,2]

# create the mesh
mesh <- inla.mesh.2d(
    randomSPDF, 
    cutoff=.1,
    max.edge=c(50, 500))
# create the projector object down to a 500 by 500 grid
proj <- inla.mesh.projector(mesh, dims=c(500, 500))

# set up the parameter values
beta0 <- -1
sigma0 <-  .6   ## Standard deviation
range0 <- 1.5 ## Spatial range
kappa0 <- sqrt(8) / range0
tau0 <- 1/(sqrt(4*pi)*kappa0*sigma0)
spde <- inla.spde2.matern(mesh)

# set up the spatial GMRF approximation, this will need to change to an
# actual matern GP later
Q <- tau0**2 * (kappa0**4 * spde$param.inla$M0 + 
                     2 * kappa0**2 *spde$param.inla$M1 + spde$param.inla$M2)
# simulate and demmean data
x_ <- as.vector(sim.AR(n=1, Q))
x <- x_ - mean(x_)

# project ont full space and plot
simValues <- mesh2DF(x)

simValues %>%
    filter(obsField) %>%
    mutate(p=invlogit(beta0 + obs)) %>%
    ggplot(aes(x, y, z=p)) +
    geom_raster(aes(fill=p)) + 
    coord_equal() +
    theme_void() + 
    scale_fill_distiller(palette = "Spectral") +
    geom_path(
        aes(long,lat, group=group, fill=NULL, z=NULL),
        color="black",
        size=.1,
        data=USDF) +
    labs(fill="Probability", title="Underlying Probability Field")

# assume equal pop weight across pixels
countyRiskDF <- simValues %>%
    filter(obsField) %>%
    group_by(id) %>%
    summarize(p=mean(invlogit(beta0 + obs), na.rm=T), cells=n())
```

For this example we will make the simplifying assumption that populations are evenly distributed in space and therefore any areal units probability of an event occurring is simply its average across space. We may then aggregate the underlying probabilities to calculate the probabilities of events at the county level shown in the map below.

```{r cmap, message=FALSE, warning=FALSE, echo=FALSE}
N <- 1200 # number of points observed

countyRiskDF %>%
    right_join(USDF, by="id", copy=T) %>%
    ggplot +
    aes(long,lat,group=group,fill=p) + 
    geom_polygon() +
    geom_path(color="black", size=.1) +
    coord_equal() + 
    theme_void() +
    scale_fill_distiller(palette = "Spectral") +
    labs(fill="Probability", title="County Level Probability")
```

In a traditional underlying latent field model approach we would estimate the underlying field by observing points over space and taking an SPDE approach, modeling the points as a function of a gaussian markov random field that closely approximates a gaussian process. To do so we need to triangulate a mesh from which we may estimate random effects from and project to any other point on the field as described in Lindgren et al. Below are a set of `r N` points and where they appear in space in relation to US counties as well as their positioning o the estimated mesh. 

```{r firstfield, echo=F, message=F, warning=F}
# Just get the observed Aproj since thats what we care about for likelihood
# dim(AprojObs)
AprojObs <- proj$proj$A[simValues$obsField,]

# create matrix for which points fall within areal unit
# creates a projection for each polygon
# dim(AprojPoly)
# table(AprojPoly[,1])
AprojPoly <- Matrix(
    sparse = T, 
    data = sapply(1:nrow(US.df@data), function(i){
        pix <- rep(0, nrow(AprojObs))
        ones <- simValues %>%
            filter(id==i) %>%
            select(obsAproj) %>%
            unlist %>%
            unname
        pix[ones] <- 1/length(ones)
        pix
    }))

runModel <- function(DFpoint=NULL, DFpoly=NULL, recompile=F, verbose=F,
                     symboic=T, draws=1000){
    model <- "pppSim"
    #compile(paste0(model, ".cpp"))
    if(is.null(DFpoly)){
        empty <- vector("integer") 
        DFpoly <- data.table(obs=empty, denom=empty, loc=empty, long=empty)
    }
    if(is.null(DFpoint)){
        empty <- vector("integer") 
        DFpoint <- data.table(obs=empty, denom=empty, lat=empty, long=empty)
    }
    AprojPoint <- inla.spde.make.A(
        mesh=mesh, 
        loc=as.matrix(select(DFpoint, long, lat)))
    Data <- list(
        yPoint=DFpoint$obs, denomPoint=DFpoint$denom, 
        yPoly=DFpoly$obs, denomPoly=DFpoly$denom, loc=DFpoly$loc,
        M0=spde$param.inla$M0, M1=spde$param.inla$M1, M2=spde$param.inla$M2,
        AprojPoint=AprojPoint, AprojPoly=AprojPoly, AprojObs=AprojObs)
    
    Params <- list(
        beta0=0, log_tau=0, log_kappa=0, z=rep(0, nrow(Q))
    )
    
    dyn.load(dynlib(model))
    startTime <- Sys.time()
    Obj <- MakeADFun(data=Data, parameters=Params, DLL=model, random="z",
                     silent=!verbose)
    Obj$env$tracemgc <- verbose
    Obj$env$inner.control$trace <- verbose
    symbolic <- T
    if(symbolic){
        nah <- capture.output(runSymbolicAnalysis(Obj))
    }
    Opt <- nlminb(
        start=Obj$par,
        objective=Obj$fn,
        gradient=Obj$gr,
        control=list(eval.max=1e4, iter.max=1e4))
    sdrep <- sdreport(Obj, getJointPrecision=TRUE)
    runtime <- Sys.time() - startTime
    if(attr(runtime, "units") == "mins"){
        runtime <- as.numeric(runtime) * 60
    }
    else if(attr(runtime, "units") == "secs"){
        runtime <- as.numeric(runtime)
    }
    zindex <- "z" == row.names(sdrep$jointPrecision)
    Qz <- sdrep$jointPrecision[zindex,zindex]
    Zdraws <- sim.AR(draws, Qz)
    Report <- Obj$report()
    zDF <- tibble(
        mu=Report$z,
        sd=apply(Zdraws, 2, sd),
        lwr=apply(Zdraws, 2, quantile, probs=.025),
        upr=apply(Zdraws, 2, quantile, probs=.975)
        )
    return(list(obj=Obj, opt=Opt, z=zDF, runtime=runtime, sd=sdrep))
}

# simulate points only
obsPoints <- spsample(US.df, N, "random")
AprojPoint <- inla.spde.make.A(mesh=mesh, loc=obsPoints)
obsDF <- data.table(long=obsPoints@coords[,1], lat=obsPoints@coords[,2]) %>%
    mutate(re=as.vector(AprojPoint %*% x), denom=rpois(N, 100)) %>%
    mutate(prob=invlogit(beta0 + re), obs=rbinom(N, denom, prob)) %>%
    cbind(select(over(obsPoints, US.df), id))

USDF %>%
    ggplot +
    aes(long,lat,group=group) + 
    geom_path(color="black", size=.1) +
    coord_equal() + 
    theme_void() +
    geom_point(
        aes(group=NULL), 
        data=obsDF,
        color="red",
        size=.5) +
    labs(title="Points Observed In US")

plot(mesh, main="Points Observed On Mesh")
points(obsPoints@coords, col="red", pch=21, cex=.2)
```

This approach has been described extensively in prior literature and is integrated into statistical modeling packages such as INLA and Template Model Builder(TMB). We use TMB for this analysis because of its flexibility for creating likelihoods however for this example INLA may be used as well and provides similar results. Unsurprisingly, the model is able to estimate the underlying field well despite observing only a limited number of points in space.

```{r firstmodel, echo=F, message=F, warning=F}
simPointsOnly <- runModel(obsDF, recompile=F, symboic=F)
estValues <- mesh2DF(simPointsOnly)

rbind(
    mutate(estValues, type="Estimated"), 
    mutate(simValues, type="True")) %>%
    filter(obsField) %>%
    ggplot(aes(x, y, z=obs)) +
    geom_raster(aes(fill = obs)) + 
    coord_equal() +
    theme_void() + 
    scale_fill_distiller(palette = "Spectral") +
    geom_path(
        aes(long,lat, group=group, fill=NULL, z=NULL),
        color="black",
        size=.1,
        data=USDF) +
    facet_wrap(~type) +
    labs(title="Estimation of Underlying Latent Field")

sample_ids <- function(df){
    dfID <- unique(select(df, id, denom))
    bind_rows(lapply(1:nrow(dfID), function(i){
        id_i <- dfID$id[i]
        denom_i <- dfID$denom[i]
        df %>%
            filter(id == id_i) %>%
            sample_n(denom_i, replace=T) %>%
            mutate(denom=1)
    }))
}


# now lets try adding in some polygons to the equation
obsPolyMixDF <- simValues %>% # start with sim field
    filter(obsField) %>% # restrict to observed filed
    mutate(p=invlogit(beta0 + obs)) %>% # claculate the p of the observed field
    select(id, p) %>% # restrict only to those two columns
    as.data.frame %>% # merge on the randomly selected polygons and their denom
    left_join(data.frame(
        id = sample(US.df$id, size=round(nrow(US.df@data)/2))) %>%
        left_join(obsDF) %>%
        group_by(id) %>%
        summarize(denom=sum(denom))) %>%
    filter(!is.na(denom)) %>%
    sample_ids %>% # sample w/ replacement from each id equal to the denom size 
    mutate(obs=rbinom(n(), size=denom, prob=p)) %>% # simulate
    select(-p) %>%
    group_by(id) %>% # aggregate to the polygon level
    summarize_all(sum) %>%
    mutate(loc=id-1) %>%
    left_join(select(countyRiskDF, id, cells))

obsPointMixDF <- obsDF %>% 
    as.data.frame %>% 
    left_join(select(obsPolyMixDF, id, cells)) %>%
    filter(is.na(cells))

AprojPointMix <- inla.spde.make.A(
    mesh=mesh, 
    loc=as.matrix(select(obsPointMixDF, long, lat)))
```

For various reasons, data is not always available to us at the point level and we may have data that comes in the form of a mix of points and areal units. To simulate this we take our original data points that we observe and remove half of them and replace them with representative samples at the county level with the same number of observed samples as we had with the point data. The total number of individuals observed is unchanged however we change the outcome of binomial events so that areal data is representative. Our new data set then comes to us in a mixed format where we observe `r nrow(obsPointMixDF)` data points that are geolocated and `r nrow(obsPolyMixDF)` which come from areal data. The map below shows where we have point and areal data where dark gray cells represent locations of area data. 

```{r mix, echo=F, message=F, warning=F}
USDF %>%
    left_join(obsPolyMixDF) %>%
    mutate(observed=as.numeric(!is.na(denom)) * .2) %>%
    ggplot +
    aes(long,lat,group=group) + 
    geom_path(color="black", size=.1) +
    geom_polygon(aes(alpha=observed)) +
    coord_equal() + 
    theme_void() +
    geom_point(
        aes(group=NULL), 
        data=obsPointMixDF,
        color="red",
        size=.5) +
    guides(alpha=FALSE) +
    ggtitle("Point Polygon Mix Data")
```

Using TMB we may estimate the same underlying random effect field that we did in the first model and evaluate polygon data as part of the data likelihood as outlined in the methods section. The consequence of this is that our underlying latent field that estimated is more pulled to the mean for locations where we have polygon data but the over all field is still well recovered as shown below.

```{r mixmodel, echo=F, message=F, warning=F}
simPPMix <- runModel(obsPointMixDF, obsPolyMixDF, recompile=F, symboic=F)
estValuesMix <- mesh2DF(simPPMix)
rbind(
    mutate(estValuesMix, type="Estimated with Mix"),
    mutate(estValues, type="Estimated with Points Only"),
    mutate(simValues, type="True")) %>%
    filter(obsField) %>%
    ggplot(aes(x, y, z=obs)) +
    geom_raster(aes(fill = obs)) + 
    coord_equal() +
    theme_void() + 
    scale_fill_distiller(palette = "Spectral") +
    geom_path(
        aes(long,lat, group=group, fill=NULL, z=NULL),
        color="black",
        size=.1,
        data=USDF) +
    facet_wrap(~type, nrow = 2) +
    ggtitle("Comparison of Model Latent Field Estimates")

```

In fact we do not need any point data at all in order to make estimates that of the underlying field because of their correlation. In the next simulation we remove all point data and only use polygon data, however, still estimate the random effects with the SPDE approach. While this may seem trivial, this approach would enable analysts to combine data from overlapping or changing areal data into a single model without having to simplify their data by combining polygons as is common in the literature. The results below show how the model is still able to approximate the underlying field well. 

```{r polymodel, echo=F, message=F, warning=F}
obsPolyDF <- simValues %>%
    filter(obsField) %>%
    mutate(p=invlogit(beta0 + obs)) %>% 
    select(id, p) %>%
    as.data.frame %>%
    left_join(US.df@data %>%
        select(id) %>%
        left_join(obsDF) %>%
        group_by(id) %>%
        summarize(denom=sum(denom))) %>%
    filter(!is.na(denom)) %>%
    sample_ids %>%
    mutate(obs=rbinom(n(), size=denom, prob=p)) %>%
    select(-p) %>%
    group_by(id) %>%
    summarize_all(sum) %>%
    mutate(loc=id-1) %>%
    left_join(select(countyRiskDF, id, cells))

simPolyOnly <- runModel(NULL, obsPolyDF, recompile=F, symboic=F)
estValuesPoly <- mesh2DF(simPolyOnly)

rbind(
    mutate(estValuesMix, type="Estimated with Mix"),
    mutate(estValues, type="Estimated with Points Only"),
    mutate(estValuesPoly, type="Estimated with Polygons Only"),
    mutate(simValues, type="True")) %>%
    filter(obsField) %>%
    ggplot(aes(x, y, z=obs)) +
    geom_raster(aes(fill = obs)) + 
    coord_equal() +
    theme_void() + 
    scale_fill_distiller(palette = "Spectral") +
    geom_path(
        aes(long,lat, group=group, fill=NULL, z=NULL),
        color="black",
        size=.1,
        data=USDF) +
    facet_wrap(~type, nrow = 2) +
    ggtitle("Comparison of Model Latent Field Estimates")

sdxm <- rbind(
    mutate(estValuesMix, type="Estimated Mix"),
    mutate(estValues, type="Estimated Points"),
    mutate(estValuesPoly, type="Estimated Polygons")) %>%
    filter(obsField) %>%
    group_by(type) %>%
    summarize(sdxm=mean(sdx)) %>% 
    select(sdxm) %>% 
    unlist %>% 
    round(2) %>%
    unname
```

How well the underlying field can be estimated is largely a function of how granular the areal data we have is, however, the benefit of this approach is that the uncertainty of the field when using polygons alone is reflected in the modeling process. Below we see a plot of the latent fields uncertainty which on average increases as we use less points and more polygons and as polygon size increases. For this simulation analysis the points only model, the mix model, and the polygon only model, had an average standard deviation across the latent field of `r sdxm[2]`, `r sdxm[1]`, and `r sdxm[3]` respectively. 

```{r uncert, echo=F, message=F, warning=F}
rbind(
    mutate(estValuesMix, type="Estimated with Mix"),
    mutate(estValues, type="Estimated with Points Only"),
    mutate(estValuesPoly, type="Estimated with Polygons Only")) %>%
    filter(obsField) %>%
    ggplot(aes(x, y, z=sdx)) +
    geom_raster(aes(fill = sdx)) + 
    coord_equal() +
    theme_void() + 
    scale_fill_distiller(palette = "Spectral") +
    geom_path(
        aes(long,lat, group=group, fill=NULL, z=NULL),
        color="black",
        size=.1,
        data=USDF) +
    facet_wrap(~type, nrow = 2) +
    labs(title="Latent Field Uncertainty", fill="Standard\nDeviation")
```